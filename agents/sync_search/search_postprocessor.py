"""
Search Post-Processor Module - Complete Working Implementation
Handles all post-processing, formatting, and enhancement of search results
"""

import logging
import re
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import uuid
from dataclasses import dataclass

logger = logging.getLogger("SearchPostprocessor")

@dataclass
class ProcessedResult:
    """Container for processed search result"""
    formatted_response: str
    metadata: Dict[str, Any]
    quality_metrics: Dict[str, Any]
    source_summary: Dict[str, Any]
    processing_info: Dict[str, Any]

class SearchResponsePostprocessor:
    """
    Handles post-processing of search responses for optimal presentation
    """
    
    def __init__(self, config_loader, persona_engine=None):
        """
        Initialize post-processor with configuration
        
        Args:
            config_loader: Configuration loader instance
            persona_engine: Optional persona engine for enhanced formatting
        """
        self.config_loader = config_loader
        self.persona_engine = persona_engine
        
        # Quality thresholds
        self.min_response_length = 200
        self.max_response_length = 5000
        self.min_source_count = 1
        
        # Formatting templates
        self.persona_templates = self._load_persona_templates()
        
        logger.info("ðŸ”§ SearchResponsePostprocessor initialized")
    
    def process_search_response(
        self,
        raw_response: str,
        persona: str,
        search_metadata: Dict[str, Any],
        correlation_id: str
    ) -> ProcessedResult:
        """
        Main post-processing method for search responses
        
        Args:
            raw_response: Raw response from search agent
            persona: Target persona for formatting
            search_metadata: Metadata from search execution
            correlation_id: Request correlation ID
            
        Returns:
            ProcessedResult containing formatted response and metadata
        """
        start_time = datetime.utcnow()
        
        try:
            # Step 1: Quality validation
            quality_check = self._validate_response_quality(raw_response, persona)
            
            # Step 2: Content enhancement
            enhanced_response = self._enhance_content(raw_response, persona, search_metadata)
            
            # Step 3: Format optimization
            formatted_response = self._apply_persona_formatting(enhanced_response, persona)
            
            # Step 4: Source attribution
            attributed_response = self._add_source_attribution(formatted_response, search_metadata)
            
            # Step 5: Final quality pass
            final_response = self._final_quality_pass(attributed_response, persona)
            
            # Generate processing metadata
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            processing_info = self._generate_processing_info(
                processing_time, correlation_id, persona
            )
            
            # Create source summary
            source_summary = self._create_source_summary(search_metadata)
            
            # Generate quality metrics
            quality_metrics = self._calculate_quality_metrics(
                final_response, search_metadata, quality_check
            )
            
            return ProcessedResult(
                formatted_response=final_response,
                metadata={
                    "persona": persona,
                    "search_metadata": search_metadata,
                    "correlation_id": correlation_id,
                    "timestamp": datetime.utcnow().isoformat()
                },
                quality_metrics=quality_metrics,
                source_summary=source_summary,
                processing_info=processing_info
            )
            
        except Exception as e:
            logger.error(f"Post-processing failed for {correlation_id}: {e}")
            # Return minimal processed response on error
            return self._create_error_response(raw_response, persona, str(e), correlation_id)
    
    def _validate_response_quality(self, response: str, persona: str) -> Dict[str, Any]:
        """Validate response quality and completeness"""
        validation = {
            "valid": True,
            "issues": [],
            "warnings": [],
            "scores": {}
        }
        
        # Length validation
        response_length = len(response.strip())
        validation["scores"]["length"] = response_length
        
        if response_length < self.min_response_length:
            validation["issues"].append(f"Response too short: {response_length} chars")
            validation["valid"] = False
        elif response_length > self.max_response_length:
            validation["warnings"].append(f"Response very long: {response_length} chars")
        
        # Content structure validation
        has_sections = bool(re.search(r'#{1,3}\s+', response))
        validation["scores"]["has_sections"] = has_sections
        
        if not has_sections:
            validation["warnings"].append("Response lacks clear section structure")
        
        # Knowledge base vs web search content validation
        has_kb_section = bool(re.search(r'knowledge\s+base|established|foundational', response, re.IGNORECASE))
        has_web_section = bool(re.search(r'latest|recent|web\s+search|developments', response, re.IGNORECASE))
        
        validation["scores"]["has_kb_content"] = has_kb_section
        validation["scores"]["has_web_content"] = has_web_section
        
        if not has_kb_section and not has_web_section:
            validation["issues"].append("Response lacks clear source differentiation")
        
        # Persona relevance check
        persona_keywords = self._get_persona_keywords(persona)
        persona_relevance = sum(1 for keyword in persona_keywords if keyword.lower() in response.lower())
        validation["scores"]["persona_relevance"] = persona_relevance / len(persona_keywords) if persona_keywords else 0
        
        if validation["scores"]["persona_relevance"] < 0.3:
            validation["warnings"].append(f"Low persona relevance for {persona}")
        
        return validation
    
    def _enhance_content(self, response: str, persona: str, metadata: Dict[str, Any]) -> str:
        """Enhance content based on persona and search metadata"""
        enhanced = response
        
        # Add persona-specific context if missing
        enhanced = self._add_persona_context(enhanced, persona)
        
        # Enhance technical depth based on persona
        enhanced = self._adjust_technical_depth(enhanced, persona)
        
        # Add actionable insights
        enhanced = self._add_actionable_insights(enhanced, persona, metadata)
        
        # Improve readability
        enhanced = self._improve_readability(enhanced)
        
        return enhanced
    
    def _apply_persona_formatting(self, response: str, persona: str) -> str:
        """Apply persona-specific formatting"""
        template = self.persona_templates.get(persona, self.persona_templates["default"])
        
        # Extract content sections
        sections = self._extract_content_sections(response)
        
        # Apply template formatting
        formatted = self._format_with_template(sections, template, persona)
        
        return formatted
    
    def _add_source_attribution(self, response: str, metadata: Dict[str, Any]) -> str:
        """Add proper source attribution to response"""
        # This would integrate with the actual search metadata to add proper citations
        sources_used = metadata.get("sources_summary", {})
        
        if sources_used:
            attribution_section = self._create_attribution_section(sources_used)
            response = f"{response}\n\n{attribution_section}"
        
        return response
    
    def _final_quality_pass(self, response: str, persona: str) -> str:
        """Final quality and formatting pass"""
        # Clean up formatting
        cleaned = self._clean_formatting(response)
        
        # Ensure proper section hierarchy
        cleaned = self._fix_section_hierarchy(cleaned)
        
        # Add final persona touches
        cleaned = self._add_final_persona_touches(cleaned, persona)
        
        # Validate and fix markdown
        cleaned = self._validate_markdown(cleaned)
        
        return cleaned
    
    def _extract_content_sections(self, response: str) -> Dict[str, str]:
        """Extract content sections from response"""
        sections = {
            "summary": "",
            "knowledge_base": "",
            "latest_developments": "",
            "synthesis": "",
            "other": ""
        }
        
        # Use regex to extract sections
        kb_match = re.search(r'#{1,3}\s*ðŸ“š.*?Knowledge.*?\n(.*?)(?=#{1,3}|\Z)', response, re.DOTALL | re.IGNORECASE)
        if kb_match:
            sections["knowledge_base"] = kb_match.group(1).strip()
        
        web_match = re.search(r'#{1,3}\s*ðŸŒ.*?Latest.*?\n(.*?)(?=#{1,3}|\Z)', response, re.DOTALL | re.IGNORECASE)
        if web_match:
            sections["latest_developments"] = web_match.group(1).strip()
        
        synthesis_match = re.search(r'#{1,3}\s*ðŸ’¡.*?Synth.*?\n(.*?)(?=#{1,3}|\Z)', response, re.DOTALL | re.IGNORECASE)
        if synthesis_match:
            sections["synthesis"] = synthesis_match.group(1).strip()
        
        # If no clear sections found, put everything in 'other'
        if not any(sections.values()):
            sections["other"] = response
        
        return sections
    
    def _format_with_template(self, sections: Dict[str, str], template: Dict[str, Any], persona: str) -> str:
        """Format content using persona template"""
        persona_display = persona.replace('_', ' ').title()
        
        formatted_parts = []
        
        # Add header
        if template.get("include_header", True):
            header = f"# ðŸ”„ SyncAI Intelligence Report: {persona_display}\n\n"
            header += f"*Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC*\n\n"
            formatted_parts.append(header)
        
        # Add executive summary if enabled
        if template.get("include_summary", True) and sections.get("synthesis"):
            summary = f"## ðŸ“‹ Executive Summary\n\n{sections['synthesis'][:300]}...\n\n"
            formatted_parts.append(summary)
        
        # Add knowledge base section
        if sections.get("knowledge_base"):
            kb_section = f"## ðŸ“š Knowledge Base Insights\n\n{sections['knowledge_base']}\n\n"
            formatted_parts.append(kb_section)
        
        # Add latest developments
        if sections.get("latest_developments"):
            web_section = f"## ðŸŒ Latest Developments\n\n{sections['latest_developments']}\n\n"
            formatted_parts.append(web_section)
        
        # Add strategic synthesis
        if sections.get("synthesis"):
            synthesis_section = f"## ðŸ’¡ Strategic Synthesis for {persona_display}\n\n{sections['synthesis']}\n\n"
            formatted_parts.append(synthesis_section)
        
        # Add other content
        if sections.get("other"):
            other_section = f"## ðŸ“„ Additional Information\n\n{sections['other']}\n\n"
            formatted_parts.append(other_section)
        
        # Add footer if enabled
        if template.get("include_footer", True):
            footer = self._create_footer(persona)
            formatted_parts.append(footer)
        
        return "".join(formatted_parts)
    
    def _create_attribution_section(self, sources_summary: Dict[str, Any]) -> str:
        """Create source attribution section"""
        attribution = "## ðŸ“š Sources\n\n"
        
        tools_used = sources_summary.get("tools_used", [])
        if "builtin::websearch" in tools_used:
            attribution += "- ðŸŒ Web Search: Latest online information and developments\n"
        
        if "builtin::rag" in tools_used:
            attribution += "- ðŸ“š Knowledge Base: Curated AI/ML information repository\n"
        
        agent_mode = sources_summary.get("agent_mode", "unknown")
        attribution += f"- ðŸ¤– Processing Mode: {agent_mode.replace('_', ' ').title()}\n"
        
        return attribution
    
    def _create_footer(self, persona: str) -> str:
        """Create footer section"""
        footer = "---\n\n"
        footer += "*This intelligence report was generated by SyncAI's persona-driven search system. "
        footer += "Information combines real-time web search with curated knowledge base content.*\n\n"
        footer += f"**Optimized for:** {persona.replace('_', ' ').title()}\n"
        footer += f"**Report ID:** {str(uuid.uuid4())[:8]}\n"
        return footer
    
    def _get_persona_keywords(self, persona: str) -> List[str]:
        """Get relevant keywords for persona validation"""
        keyword_map = {
            "devops_engineer": ["infrastructure", "deployment", "scaling", "cost", "MLOps", "container", "GPU"],
            "software_engineer": ["API", "framework", "SDK", "development", "integration", "coding", "library"],
            "ai_engineer": ["model", "training", "architecture", "research", "neural", "transformer", "optimization"],
            "product_owner": ["features", "user", "market", "adoption", "product", "roadmap", "metrics"],
            "product_manager": ["strategy", "business", "revenue", "market", "competitive", "opportunity", "ROI"]
        }
        return keyword_map.get(persona, [])
    
    def _add_persona_context(self, response: str, persona: str) -> str:
        """Add persona-specific context to response"""
        persona_contexts = {
            "devops_engineer": "From an infrastructure and operational perspective",
            "software_engineer": "From a development and integration perspective", 
            "ai_engineer": "From a research and technical implementation perspective",
            "product_owner": "From a product development and user value perspective",
            "product_manager": "From a business strategy and market perspective"
        }
        
        context = persona_contexts.get(persona, "")
        if context and context.lower() not in response.lower():
            # Add context where appropriate
            pass  # Implementation would intelligently insert context
        
        return response
    
    def _adjust_technical_depth(self, response: str, persona: str) -> str:
        """Adjust technical depth based on persona"""
        technical_personas = ["ai_engineer", "software_engineer", "devops_engineer"]
        business_personas = ["product_manager", "product_owner"]
        
        if persona in technical_personas:
            # Could enhance with more technical details
            pass
        elif persona in business_personas:
            # Could simplify technical language
            pass
        
        return response
    
    def _add_actionable_insights(self, response: str, persona: str, metadata: Dict[str, Any]) -> str:
        """Add actionable insights based on persona"""
        # This would analyze the content and add persona-specific actionable recommendations
        return response
    
    def _improve_readability(self, response: str) -> str:
        """Improve overall readability"""
        # Fix paragraph spacing
        improved = re.sub(r'\n{3,}', '\n\n', response)
        
        # Ensure proper list formatting
        improved = re.sub(r'\n-\s+', '\n- ', improved)
        improved = re.sub(r'\n\*\s+', '\n* ', improved)
        
        # Fix spacing around headers
        improved = re.sub(r'\n(#{1,6})', r'\n\n\1', improved)
        improved = re.sub(r'(#{1,6}.*?)\n([^\n#])', r'\1\n\n\2', improved)
        
        return improved.strip()
    
    def _clean_formatting(self, response: str) -> str:
        """Clean up formatting issues"""
        # Remove excessive whitespace
        cleaned = re.sub(r'\s+', ' ', response)
        cleaned = re.sub(r'\n\s*\n', '\n\n', cleaned)
        
        # Fix markdown formatting
        cleaned = re.sub(r'\*\*\s+', '**', cleaned)
        cleaned = re.sub(r'\s+\*\*', '**', cleaned)
        
        return cleaned.strip()
    
    def _fix_section_hierarchy(self, response: str) -> str:
        """Ensure proper section hierarchy"""
        # This would fix markdown header hierarchy issues
        return response
    
    def _add_final_persona_touches(self, response: str, persona: str) -> str:
        """Add final persona-specific touches"""
        return response
    
    def _validate_markdown(self, response: str) -> str:
        """Validate and fix markdown formatting"""
        # Basic markdown validation and fixing
        return response
    
    def _calculate_quality_metrics(
        self, 
        response: str, 
        metadata: Dict[str, Any], 
        quality_check: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Calculate quality metrics for the processed response"""
        return {
            "response_length": len(response),
            "has_proper_sections": quality_check.get("scores", {}).get("has_sections", False),
            "persona_relevance_score": quality_check.get("scores", {}).get("persona_relevance", 0),
            "source_coverage": quality_check.get("scores", {}).get("has_kb_content", False) and quality_check.get("scores", {}).get("has_web_content", False),
            "quality_issues": len(quality_check.get("issues", [])),
            "quality_warnings": len(quality_check.get("warnings", [])),
            "overall_score": self._calculate_overall_quality_score(quality_check)
        }
    
    def _calculate_overall_quality_score(self, quality_check: Dict[str, Any]) -> float:
        """Calculate overall quality score"""
        scores = quality_check.get("scores", {})
        issues = len(quality_check.get("issues", []))
        warnings = len(quality_check.get("warnings", []))
        
        base_score = 1.0
        
        # Deduct for issues and warnings
        base_score -= (issues * 0.2)
        base_score -= (warnings * 0.1)
        
        # Add for positive indicators
        if scores.get("has_sections", False):
            base_score += 0.1
        
        if scores.get("persona_relevance", 0) > 0.5:
            base_score += 0.1
        
        return max(0.0, min(1.0, base_score))
    
    def _create_source_summary(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Create summary of sources used"""
        return {
            "tools_used": metadata.get("tools_used", []),
            "agent_mode": metadata.get("agent_mode", "unknown"),
            "search_strategy": metadata.get("search_strategy", "unknown"),
            "sources_available": metadata.get("response_available", False)
        }
    
    def _generate_processing_info(self, processing_time: float, correlation_id: str, persona: str) -> Dict[str, Any]:
        """Generate processing information"""
        return {
            "processing_time_seconds": processing_time,
            "correlation_id": correlation_id,
            "persona": persona,
            "processor_version": "1.0.0",
            "processed_at": datetime.utcnow().isoformat()
        }
    
    def _create_error_response(self, raw_response: str, persona: str, error: str, correlation_id: str) -> ProcessedResult:
        """Create minimal response when processing fails"""
        return ProcessedResult(
            formatted_response=raw_response or f"Processing failed for {persona}. Error: {error}",
            metadata={
                "persona": persona,
                "correlation_id": correlation_id,
                "error": error,
                "timestamp": datetime.utcnow().isoformat()
            },
            quality_metrics={"error": True, "overall_score": 0.0},
            source_summary={"error": True},
            processing_info={
                "processing_time_seconds": 0,
                "error": error,
                "processed_at": datetime.utcnow().isoformat()
            }
        )
    
    def _load_persona_templates(self) -> Dict[str, Dict[str, Any]]:
        """Load persona-specific formatting templates"""
        return {
            "devops_engineer": {
                "include_header": True,
                "include_summary": True,
                "include_footer": True,
                "style": "technical_operational",
                "focus": ["infrastructure", "cost", "scalability"]
            },
            "software_engineer": {
                "include_header": True,
                "include_summary": True,
                "include_footer": True,
                "style": "technical_practical",
                "focus": ["implementation", "tools", "frameworks"]
            },
            "ai_engineer": {
                "include_header": True,
                "include_summary": True,
                "include_footer": True,
                "style": "research_technical",
                "focus": ["research", "architectures", "innovations"]
            },
            "product_owner": {
                "include_header": True,
                "include_summary": True,
                "include_footer": True,
                "style": "product_focused",
                "focus": ["features", "users", "market"]
            },
            "product_manager": {
                "include_header": True,
                "include_summary": True,
                "include_footer": True,
                "style": "strategic_business",
                "focus": ["strategy", "business", "market"]
            },
            "default": {
                "include_header": True,
                "include_summary": False,
                "include_footer": True,
                "style": "balanced",
                "focus": []
            }
        }

class ResponseQualityAnalyzer:
    """
    Analyzes and scores response quality
    """
    
    def __init__(self):
        self.quality_criteria = {
            "completeness": 0.3,
            "relevance": 0.25,
            "readability": 0.2,
            "source_attribution": 0.15,
            "actionability": 0.1
        }
    
    def analyze_response(self, response: str, persona: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Comprehensive response quality analysis"""
        scores = {}
        
        # Completeness analysis
        scores["completeness"] = self._analyze_completeness(response)
        
        # Relevance analysis
        scores["relevance"] = self._analyze_relevance(response, persona)
        
        # Readability analysis
        scores["readability"] = self._analyze_readability(response)
        
        # Source attribution analysis
        scores["source_attribution"] = self._analyze_source_attribution(response, metadata)
        
        # Actionability analysis
        scores["actionability"] = self._analyze_actionability(response, persona)
        
        # Calculate weighted overall score
        overall_score = sum(
            scores[criterion] * weight 
            for criterion, weight in self.quality_criteria.items()
        )
        
        return {
            "scores": scores,
            "overall_score": overall_score,
            "quality_grade": self._get_quality_grade(overall_score),
            "recommendations": self._generate_recommendations(scores)
        }
    
    def _analyze_completeness(self, response: str) -> float:
        """Analyze response completeness"""
        indicators = [
            bool(re.search(r'knowledge.*base', response, re.IGNORECASE)),
            bool(re.search(r'latest.*developments', response, re.IGNORECASE)),
            bool(re.search(r'#{1,3}', response)),  # Has sections
            len(response) > 500,  # Substantial content
            bool(re.search(r'synth|recommend|next.*step', response, re.IGNORECASE))
        ]
        return sum(indicators) / len(indicators)
    
    def _analyze_relevance(self, response: str, persona: str) -> float:
        """Analyze persona relevance"""
        persona_keywords = {
            "devops_engineer": ["infrastructure", "deployment", "ops", "scaling", "cost"],
            "software_engineer": ["api", "framework", "development", "code", "integration"],
            "ai_engineer": ["model", "research", "training", "architecture", "neural"],
            "product_owner": ["product", "feature", "user", "market", "roadmap"],
            "product_manager": ["business", "strategy", "revenue", "competitive", "market"]
        }
        
        keywords = persona_keywords.get(persona, [])
        if not keywords:
            return 0.5  # Neutral if no keywords defined
        
        matches = sum(1 for keyword in keywords if keyword in response.lower())
        return min(1.0, matches / len(keywords))
    
    def _analyze_readability(self, response: str) -> float:
        """Analyze response readability"""
        indicators = [
            bool(re.search(r'#{1,3}', response)),  # Has headers
            bool(re.search(r'^\s*[-*]\s+', response, re.MULTILINE)),  # Has lists
            len(response.split('\n\n')) > 2,  # Multiple paragraphs
            not bool(re.search(r'.{200,}', response.replace('\n', ' '))),  # No overly long sentences
            bool(re.search(r'[ðŸ“šðŸŒðŸ’¡ðŸ”âš¡ðŸ“‹ðŸ› ï¸]', response))  # Has emojis for visual appeal
        ]
        return sum(indicators) / len(indicators)
    
    def _analyze_source_attribution(self, response: str, metadata: Dict[str, Any]) -> float:
        """Analyze source attribution quality"""
        has_sources_section = bool(re.search(r'#{1,3}.*source', response, re.IGNORECASE))
        mentions_tools = any(tool in response.lower() for tool in ['web search', 'knowledge base', 'rag'])
        metadata_has_sources = bool(metadata.get("sources_summary"))
        
        indicators = [has_sources_section, mentions_tools, metadata_has_sources]
        return sum(indicators) / len(indicators)
    
    def _analyze_actionability(self, response: str, persona: str) -> float:
        """Analyze actionability of response"""
        action_indicators = [
            bool(re.search(r'recommend|suggest|should|consider', response, re.IGNORECASE)),
            bool(re.search(r'next.*step|action.*item|implement', response, re.IGNORECASE)),
            bool(re.search(r'strategy|approach|method', response, re.IGNORECASE)),
            bool(re.search(r'how.*to|guide|tutorial', response, re.IGNORECASE))
        ]
        return sum(action_indicators) / len(action_indicators)
    
    def _get_quality_grade(self, score: float) -> str:
        """Convert score to quality grade"""
        if score >= 0.9:
            return "A+"
        elif score >= 0.8:
            return "A"
        elif score >= 0.7:
            return "B+"
        elif score >= 0.6:
            return "B"
        elif score >= 0.5:
            return "C+"
        elif score >= 0.4:
            return "C"
        else:
            return "D"
    
    def _generate_recommendations(self, scores: Dict[str, float]) -> List[str]:
        """Generate improvement recommendations"""
        recommendations = []
        
        if scores["completeness"] < 0.7:
            recommendations.append("Add more comprehensive coverage of both knowledge base and latest developments")
        
        if scores["relevance"] < 0.6:
            recommendations.append("Increase persona-specific content and terminology")
        
        if scores["readability"] < 0.7:
            recommendations.append("Improve formatting with better headers, lists, and visual elements")
        
        if scores["source_attribution"] < 0.5:
            recommendations.append("Add clear source attribution and mention search methodology")
        
        if scores["actionability"] < 0.6:
            recommendations.append("Include more actionable recommendations and next steps")
        
        return recommendations

class PersonaResponseFormatter:
    """
    Specialized formatter for different persona types
    """
    
    def __init__(self):
        self.formatters = {
            "devops_engineer": self._format_devops,
            "software_engineer": self._format_software_engineer,
            "ai_engineer": self._format_ai_engineer,
            "product_owner": self._format_product_owner,
            "product_manager": self._format_product_manager
        }
    
    def format_for_persona(self, content: str, persona: str) -> str:
        """Format content specifically for persona"""
        formatter = self.formatters.get(persona, self._format_default)
        return formatter(content)
    
    def _format_devops(self, content: str) -> str:
        """Format for DevOps Engineer persona"""
        # Add operational focus, cost implications, scalability considerations
        return self._add_operational_context(content)
    
    def _format_software_engineer(self, content: str) -> str:
        """Format for Software Engineer persona"""
        # Add implementation details, code examples, integration guidance
        return self._add_technical_implementation_context(content)
    
    def _format_ai_engineer(self, content: str) -> str:
        """Format for AI Engineer persona"""
        # Add research context, technical depth, methodology focus
        return self._add_research_context(content)
    
    def _format_product_owner(self, content: str) -> str:
        """Format for Product Owner persona"""
        # Add user value, feature implications, roadmap considerations
        return self._add_product_context(content)
    
    def _format_product_manager(self, content: str) -> str:
        """Format for Product Manager persona"""
        # Add business strategy, market implications, competitive analysis
        return self._add_business_context(content)
    
    def _format_default(self, content: str) -> str:
        """Default formatting"""
        return content
    
    def _add_operational_context(self, content: str) -> str:
        """Add operational context for DevOps"""
        # Implementation would add operational perspective
        return content
    
    def _add_technical_implementation_context(self, content: str) -> str:
        """Add technical implementation context"""
        # Implementation would add development perspective
        return content
    
    def _add_research_context(self, content: str) -> str:
        """Add research and technical context"""
        # Implementation would add research perspective
        return content
    
    def _add_product_context(self, content: str) -> str:
        """Add product development context"""
        # Implementation would add product perspective
        return content
    
    def _add_business_context(self, content: str) -> str:
        """Add business strategy context"""
        # Implementation would add business perspective
        return content